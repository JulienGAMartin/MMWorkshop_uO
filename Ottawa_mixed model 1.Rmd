---
title: "Simple mixed models and random slope models using LME4"
author: "Alastair Wilson and Tom Houslay"
date: "9th Nov 2019"
output: html_document
---
```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE)

```

## 1. Introduction

### Overview

This tutorial is intended to get you started fitting some simple mixed models with so called 'random intercepts', and then goes on to explore how random regression (or 'random slope') models can be used to extend our understanding further. The tutorial is derived from one that accompanied our 2017 Behavioral Ecology paper, "[Avoiding the misuse of BLUP in behavioral ecology](https://doi.org/10.1093/beheco/arx023)". Here, you will be working through  a simplified version in which we've taken more time to cover the basic mixed models and don't cover  multivariate models which were really the main point of that paper. So if you find this material interesting and/or useful, the original version will be worth a work through to help you break into multivariate mixed models! Here we will:

* Learn how to fit - and interpret the results of - a simple univariate mixed effect model  
* See how to add fixed and random effects to your model, and to test their significance in the normal frequentists sense  
* Use random regression or random slope models as extensions of the simple mixed model. We will do this in a hypothetical investigation of  behavioural plasticity, adopting a 'reaction norm perspective' and asking whether individuals differ in phenotypic plasticity (a phenomenon sometimes called IxE)
*Try and flag some common pitfalls with random regression models, and in particular show you why you need to be careful interpreting effect sizes and why you need to think a bit about if (and how) to scale and centre covariates.  
* Finally, as a lead in why you really should learn about multivariate models after this, we will highlight how character state views of plasticity are really just approximations of multivariate 'character state' approaches. This is a bit mind blowing at first, but once you understand you will start to see just how far the rabbit hole goes... 

We illustrate these things using the `R` package `LME4` which is widely used and great for simple mixed models. For slightly more complex models, including multivariate ones, generalised models, and random effects of things like shared space, pedigree, phylogeny we tend to use different packages like `MCMCglmm` (which is Bayesian, look at Jarrod Hadfield's excellent course notes, available at the [MCMCglmm CRAN page](https://cran.r-project.org/web/packages/MCMCglmm/index.html)) or ASRemlR (which is likelihood based/frequentist but sadly is not free).

We also use various methods for manipulating and visualising data frames using the `tidyverse` package (including `tidyr`, `dplyr`, `ggplot2` etc) --- more details on their use can be found at [http://r4ds.had.co.nz/](http://r4ds.had.co.nz/). This aspect of the tutorial is all Tom's doing. Alastair likes mixed models but doesn't really know what these functions do that the base R functions don't (besides make Tom happy). Tom also loves plotting things, anything and everything - even when it really serves no purpose. Alastair rarely bothers plotting things, even when he really should do. We've tried to find a happy medium here:  

Further data sets and tutorials written by Tom can be found at [https://tomhouslay.com/tutorials/](https://tomhouslay.com/tutorials/).

### Packages required

There are several packages that you must have installed in `R` prior to starting this tutorial:

* `lme4`
* `lmerTest`
* `nadiv`
* `tidyverse`
* `broom`
* `gridExtra`
* `lattice`

### Some biology first - the ferocious wild Haggis of the Scottish Highlands 
Because people tend to mix up the only two famous poems Robbie Burns ever wrote, you may have heard the haggis described as a 'Wee, sleekit, cowrin, tim'rous beastie'. In fact that's not from 'Address to a haggis', it's from his other poem where he was talking about a mouse.  

![A male haggis in the wild (thanks to Emma Wood, http://www.ewood-art.co.uk/)](haggis.jpg)


Unlike the relatively placid mouse, the haggis (*Haggis scoticus*) is a radge wee beastie - with males displaying escalated aggression towards rivals that enter their territories.

We want to know:  
* If aggressiveness differs among individuals  
* If aggressive behaviour is plastic  
* If individuals differ in their plasticity   

With respect to plasticity, we will focus on rival size as an 'environment'. Common sense, and animal-contest theory, suggest a small male would be wise not to escalate an aggressive contest against a larger, stronger rival. However, there are reports in the (very small) haggis literature that they get more aggressive as rival size increases. Those reports are based on small sample sizes and uncontrolled field observations by some bloke who'd just been on a whisky distiller "tour". So it may be true but we wanted to explore this possibility under more controlled experimental conditions.  

### Experimental design -  what is the structure of the data we have?

Here, we have measured aggression in a population of wild haggis. We brought some (n=80) haggis into the lab, tagged them so they were individually identifiable, then repeatedly observed their aggression when presented with  model 'intruders'. There were three models; one of average male haggis size (calculated as the population mean body length), one that was build to be 1 standard deviation below the population mean, and one that was 1 standard deviation above.  

Data were collected on all males in two block of lab work. Within each block, each male was tested 3 times, once against an 'intruder' of each size. The test order in which each male experienced the three instruder sizes was randomised in each block. The body size of all focal individuals was measured at the beginning of each block so we know that too (and have two separate measures per individual). 

Let's imagine all the haggis were then released back to the wild, and through some cunning and no doubt laborious combination of fieldwork and paternity analysis of baby haggis born the next year we have measured the number of offspring produced by each male in our experiement. Fortunately for us the haggis only survives for one breeding season so this is actually a great measure of lifetime fitness. We could use  this to estimate natural selection on the repeatable (individual) part of  aggressiveness, or even (if you REALLY wanted to) selection on plasticity of aggressiveness. We won't look at selection today but the full tutorial on Tom's website explains how to do that. 

### Load libraries and inspect data

First we load required libraries
```{r loadlibs, message=FALSE, results='hide', warning=FALSE}

library(lme4)
library(lmerTest)
library(tidyverse)
library(broom)
library(nadiv)
library(gridExtra)
library(lattice)

```

Then let's load the data file and make sure we understand what it contains
```{r load_data_aggr, message=FALSE, results='hide', warning=FALSE}

haggis <- read_csv("aggression.csv")

```

You can use `summary(haggis)` to get an overview of the data and/or `head(haggis)` to see the structure in the first few lines. This data frame has 6 variables:

* Individual __ID__
* Experimental __Block__, denoted for now as a continuous variable with possible values of -0.5 (first block) or +0.5 (second block)
* Individual __body_size__, as measured at the start of each block
* The repeat number for each behavioural test, __assay_rep__
* Opponent size (__opp_size__), in standard deviations from the mean (i.e., -1,0,1)
* __aggression__, our behavioural trait, measured 6 times in total per individual (2 blocks of 3 tests)
* __fitness__, our measure of mating success, with a single value for each individual representing the number of offspring produced

  
## 2. Do male haggis differ in aggressiveness? Your first mixed model

A sensible researcher would probably take the time to do some exploratory data plots here, but if you - like Alastair - just want the answer without all the stupid pictures, you will jump straight in . So let's wtrite a mixed model. This one is going to have no fixed effects except the mean, and just one random effect - individual identity.  

Why, so simple? Because we simply want to partition variance around the mean into a component that among-individual variance and one that is within-individual variance.


```{r mod1}

lmer_1 <- lmer(aggression ~ 1 +  (1|ID), data = haggis)

```

This throws a warning... something about "singularities". Ignore that for a moment and *summarise* the model (every time we say that we just mean use `summary(model_name)` like this: 


```{r mod1_summary}

summary(lmer_1)

```

In the summary you will  find a table of fixed effects. The intercept (here the mean) is about 9 and is significantly >0 - fine, but not very interesting to us. You will also find a random effect table that contains estimates of the among individual (ID) and residual variances. The former is estimated as zero. 

In fact this is what the cryptic warning was about: in most situations the idea of a random effect explaining less than zero variance is not sensible (strangely there are exception!).  So by default the variance estimates are constrained to lie in positive parameter space. Here in trying to find the maximum likelihood solution for among-individual variance, our model has run up against this constraint. 

We can  test the statistical significance of the random effect using the `ranova()` command in `lmerTest`. This function is actually doing a *likelihood ratio test* of the random effect. The premise of which is that twice the difference in log-likelihood of the full and reduced (i.e. with the random effect dropped) is itself distributed as chisquared with DF equal to the number of parameters dropped (here 1). Actually, there is a good argument that this is too conservative, but let's not get into that now. Given an effect size of zero, the outcome of this test probably won't suprise you.


```{r mod1_ranova} 

ranova(lmer_1)

```
There is apparently no among-individual variance in aggressiveness.  
The estimate of zero means the **repeatability** is also zero (i.e. Vind/(Vind+Vres))

**And that concludes your first mixed model. A pretty underwhelming experience I'm sure you will agree!**
  
  
  
  
  
## 3. Do male haggis differ in aggressiveness? A better mixed model

The answer we got from our first model is **not** wrong, but as models go this one is fairly rubbish. In fact we have explained no variation at all as we have no fixed effects (except the mean) and our random effect variance is zero. We woud have seen just how pointless this model was if we'd plotted it

```{r mod1_plot} 

plot(lmer_1)

```

So we can probably do better at modelling the data, which may or may not change our view on whether there is any real variation among males in aggressiveness.

For instance, an initial plot of the phenotypic data against opponent size indicates that - as predicted - there is a general increase in aggression with opponent size (points are lightly jittered on the x-axis to show the spread of data a little better):

```{r plot_aggr, echo=TRUE, fig.height=4, fig.width=5, fig.align='center'}

ggplot(haggis, aes(x = opp_size, y = aggression)) +
  geom_jitter(alpha = 0.5,
              width = 0.05) +
  scale_x_continuous(breaks = c(-1,0,1)) +
  labs(x = "Opponent size (SD)",
       y = "Aggression") +
  theme_classic()

```

You can see the same thing from a quick look at the population means for aggression at opponent size. Here we do it with some gratuitous use of the `kable` function that apparently makes nice tables. 

```{r mean_aggr, echo=TRUE}

haggis %>% 
  group_by(opp_size) %>% 
  summarise(mean_aggr = mean(aggression)) %>% 
  kable(digits = 2)

```

So, interestingly there does appear to be plasticity of aggression with changing size of the model opponeont. But other things may explain variation in aggressiveness too - what about block for instance? Block effects may not be teh subject of any biologically interesting hypotheses, but accounting for any differences between blocks could remove noise. 

There may also be systematic change in behaviour as a male experiences more repeat observations (i.e. exposure to the model). Do they get sensitised or habituated to the model intruder for example?

So let's  run a mixed model with the same random effect of individual, but with a fixed effects of opponent size (our predictor of interest), focal male body size (mean-centred and scaled), assay repeat (mean-centred), and experimental block. 

```{r mod2,results='hide'}

lmer_2 <- lmer(aggression ~ opp_size  + block + (1|ID), data = haggis)

```

Run a few diagnostic plots before we look at the answers - hopefully they will look OK

```{r mod2_plots, eval=FALSE}

plot(lmer_2)                 #plot predicted against fitted        
qqnorm(residuals(lmer_2))    #a q-q plot
hist(resid(lmer_2))          #are the residuals roughly Gaussian? 
 
```

### What do you conclude? 

**Now summarise this model. We will pause here for you to think about and discuss a few things:**  
* What can you take from the fixed effect table?  
* How do you interpret the intercept now that there are other effects in the model?   
* What would happen if we scaled our fixed covariates differently? Why?  

**Try tweaking the fixed part of your model, we will discuss the consequences in a minute:  **
* What happens if you add more fixed effects? Try it! 
* Could focal body size also matter? If so, should you rescale before adding it to the model? 
* Sould you add interactions (e.g. block:opp_size)? 
* Should you drop non-significant fixed effects?  

**Having changed the fixed part of your model, do the variance estimates change at all?   ** 
* Is among-individual variance always estimated as zero regardless of fixed effects?      
* Is among-individual variance significant with some fixed effets structures but not others?    

### What is the repeatability? 

In our first model among-individual variance was zero, so R was zero. If we have a different model of aggression and get a non-zero value of the random effect variance, we can obviously calculate a repeatability estimate (R). So we are all working from the same starting point, let's all stick with a common set of fixed effects from here on: 
```{r mod3}

lmer_3 <- lmer(aggression ~ opp_size + scale(body_size) + 
                 scale(assay_rep, scale = FALSE) + block + 
                  (1|ID),
                data = haggis)
summary(lmer_3)

```
So we'd probably calculate R using the individual and residual variance simply as 
```{r mod3_Rcalc}

R<-0.02538/(0.02538+0.58048) # R=Vind/Vtotal =Vind/(Vind+Vresidual)
R    

```
Which yields an estimate of approximately R=4%. Strictly speaking we should make clear this a **conditional repeatability** estimate.  

Conditional on what you might ask...  on the fixed effects in your model. So our best estimate of 4% refers to the proportion of variance in aggressiveness *not explained by fixed effects* that is explained by individual identity. This isn't much and still won't be significant, but illustrates the point that conditional repeatabilities often have a tendency to go up as people explain more of the residual variance by adding fixed effects. This is fine and proper, but can mislead the unwary reader. 
It also means that decisions about which fixed effects to include in your model need to be based on how you want to interpret R not just on, for instance, whether fixed effects are deemed significant. 

### A quick note on uncertainty 
Using `lmer` in the package `LME4` there isn't a really simple way to put some measure of uncertainty (SE or CI) on derived parameters like repeatabilities. This is a bit annoying. Such things are more easily done with other mixed model packages like `mcmcglmm` and `asreml` which are a bit more specialist. If you are using `lmer` for models you want to publish then you could look into the package `rpt`. This  acts as a 'wrapper' for `lmer` models and adds some nice functionality including options to boostrap confidence intervals. Regardless, of how you do it, if you want to put a repeatability in one of your papers as a key result - it really should be accompanied by a measure of uncertainty just like any other effect size estimate.

## 4. An easy way to mess up your mixed models 
We will try some more advanced mixed models in a moment to explore plasticity in aggressiveness a bit more. First let's quickly look for among-individual variance in focal body size. Why not? We have the data handy, everyone says morphological traits are very repeatable and - lets be honest - who wouldn't like to see a small P value after striking out with aggressiveness.

Include a random effect of ID as before and maybe a fixed effect of block, just to see if the beasties were growing a bit between data collection periods.

```{r mod_size_wrong, results='hide'}

lmer_size <- lmer(body_size ~ block + (1|ID),
                data = haggis)

```
Summarise and test the random effect. **What might you conclude, and why would this be foolish?**
#  
#  
#  
#  
#  
#  
#  
#  
 
Hopefully you spotted the problem here. You have fed in a data set with 6 records per individual (with 2 sets of 3 identical values per haggis), when you know size was only measured twice in reality. This means you'd expect to get a (potentially very) upwardly biased estimate of R and a (potentially very) downwardly biased P value when testing among-individual variance. In fact Tom set up the data simulation in such a way that size is so massively repeatable we probably get almost the same answer when we do it correctly.  We can pruning to the two actual observations per haggis by just selecting the first assay in each block.

```{r mod_size_right, results='hide'}

haggis2<-haggis[haggis$assay_rep==1,]
lmer_size2 <- lmer(body_size ~   block +  (1|ID),
                data = haggis2)

```
Summarise and test your random effect and you'll see the qualitative conclusions will actually be very similar using the pruned data set. Of course this won't generallty but be true, so just be careful. Mixed models are intended to help you model repeated measures data with non-independence, but they won't get you out of trouble if you mis-represent the true structure of observations on your dependent variable. 


## 5. Random Regression mixed models: Do male haggis differ in plasticity of aggressiveness? 

Our simple mixed model of aggression suggest there is little among-individual variation. Including fixed effects gives us some useful insights but doesn't really change that view much. This model is 'good' in the sense that it answers the question we started with, and there are no massive violations of the assumptions that we are aware of. However, this doesn't mean the model 'fits the data' very well. So we could go on to explore further.


### Let's start by seeing just how well the `lmer_3` model does fit the observed data
We will use the handy `augment` function from the `broom` package to get model predictions for each of our observations. Below, we plot the raw data for each individual in one panel, with the fitted slopes in a second panel. Because we have 2 blocks as fixed effects, for ease of presentation we have selected only one of the blocks for this plot (if you like, you can check the other blocks to reassure yourself that there is little overall difference - another way would be to calculate predictions while averaging over block effects).


```{r plot_aggr_ri, fig.height=4, fig.width=5, fig.align='center'}

augment(lmer_3) %>% 
  select(ID, block, opp_size, .fitted, aggression) %>% 
  filter(block == -0.5) %>% 
  gather(type, aggression,
         `.fitted`:aggression) %>% 
  ggplot(., aes(x = opp_size, y = aggression, group = ID)) +
  geom_line(alpha = 0.3) +
  theme_classic() +
  facet_grid(.~type)

```

This illustrates how useful it can be to use model predictions to see whether the model actually fits the individual-level data well or not. Here all the diagnostic plots looked fine, and the model captures the mean plasticity (i.e. increase in average aggression with opponent size), but apart from that  it really doesn't fit the actual data very well at all. 


### Behavioural plasticity

One way we might (or might not) get a better fitting model is to allow individual not just to differ in mean aggressiveness (by including a random intercept), but also in plasicity. We can do that using a random slopes model in which each individual can have its own slope represneting the way in which aggressiveness changes plastically with opponent size. These models are also known as *random regression mixed models*. 

In this section, we will look at how one can test for and estimate individual variation in behavioural plasticity --- which is also  known as individual by environment interaction (IxE). 

We will assume reaction norms can be modelled as linear (first order) functions of the environmental variable *x* (in our case opponent size).IxE is present if individuals differ in their slopes. In fact we don't need to stick with first order linear regressions (so-called random slope models). We could add in random quadratic (and/or cubic, quartic etc) terms if we wanted and had enough data. Quantitative geneticists have done that quite a bit to allow more flexible assumptions about reaction norm shape. In practice behavioural ecologists have largely stuck to straight line models so far.    

### Random regression in lme4
So let's fit the model, keep fixed effects the same but now we specify the random effect slightly differently 
```{r aggr_rr, results=FALSE}

lmer_3_rr <- lmer(aggression ~ opp_size + scale(body_size) + 
                 scale(assay_rep, scale = FALSE) + block + 
                  (opp_size|ID),
                data = haggis)

```
Some diagnostic plots perhaps?
```{r aggr_rr_plotres, eval=FALSE}

plot(lmer_3_rr)
qqnorm(residuals(lmer_3_rr))
hist(resid(lmer_3_rr))
```

Happy? If so let's be like Tom this time and visualise our predicted fit before we even look at the summary of this model. Again, this is just looking at a single data block, but you can change the `filter` specification to check predictions with the other):

```{r plot_aggr_rr, fig.height=4, fig.width=5, fig.align='center'}

augment(lmer_3_rr) %>% 
  select(ID, block, opp_size, .fitted, aggression) %>% 
  filter(block == -0.5) %>% 
  gather(type,aggression,
         `.fitted`:aggression) %>% 
  ggplot(., aes(x = opp_size, y = aggression, group = ID)) +
  geom_line(alpha = 0.3) +
  theme_classic() +
  facet_grid(.~type)

```

This certainly seems better from a quick look, it is fitting differences in both individual intercept and slope to produce a set of predictions that look at least a bit more like the pattern of observations. We can test whether this random regression model is a significant improvement on the  model fit using the overloaded `anova` function in R. In this instance `anova` will actually perform a likelihood ratio test (LRT) again. So many different ways to skin a cat in R aren't there? Why some idiot decided to use `anova` as a generic name for comparing models in R is one of the great mysteries of life but we can't do anything about it now. Oh well.

```{r aggr_lmer_LRT_code, eval=FALSE}

anova(lmer_3_rr, lmer_3)

```

We can see here that the LRT uses a chi-square test with 2 degrees of freedom, and indicates that the random slopes model shows a statistically significant improvement in model fit. The 2df are because there are two additional (co)variance terms estimated in the random regression model: a variance term for individual slopes, and the covariance (or correlation) between the slopes and intercepts. 

### Be careful to include a slope-intercept covariance in your model
As a slight aside, different mixed model packages use different syntax to specify these random regression models. It is quite possible in many of them to specify models where individual intercepts and slopes can both vary, but (by accident or design) there is no covariance/correlation fitted between them. To a reasonable first approximation there are **no situations when such a model is biologically appropriate** as it puts very strong constraints on the patterns of possible reaction norms. Some people try to argue they can drop the covariance if it is not 'significant'. This argument is almost certainly foolish. We'll come back to why this covariance/correlation is a bit tricky later.

### Back to the task at hand
Let's look at the random and fixed effects parameters, via the model summary:

```{r aggr_lmer_summary, results=FALSE}

summary(lmer_3_rr)

```

From the fixed effects, we should see (as before) that there is a strong, significant positive effect of opponent size on aggression (1.05 SE 0.06, t = 17.15) --- so there is population-level plasticity at a rate anticipated from our earlier plots of raw data. Remember the model opponents were sized to be at the population mean body size +/- 1 standard deviation. So aaggression increases (on average) at a rate of 1 unit per standard deviation increase in opponent size. Or - if you prefer at a rate of 1 unit per 3KG increase in opponent size since, using the focal weights:

```{r aggr_lmer_mnPLAST, results=FALSE}

sd(haggis$body_size)

```

Interpreting the random effects requires some care. The **intercept variance** is the among-individual variance in aggression where x (here, opponent size) = 0. Here this corresponds to an average sized opponent because of the way we scaled the environmental covariate. This was deliberate. By making zero the mean (or median, or centre of the range etc) of x, the among-individual intercept variance becomes the among-individual variance of aggression in an 'average' environment.  

The **slope variance** is a bit tricky: intercepts are in *units of aggression* but slopes are in *units of aggression/units of opponent size*. Consequently the intercept and slope variances are in different units and you cannot directly compare their magnitudes. 

The **slope-intercept correlation** is even trickier. Here is strongly positive, but what does that actually mean? It means that **at *x* = 0** individuals with more positive intercept deviations also have more positive slopes. Look again at your plot of the model predictions and you'll hopefully see this is the case.  

But what would have happened if we had scaled our opponent size variable differently. What if we made it so 0 was a small model opponent, 1 was average and 2 was large? Model predictions and fit would have been identical, but the random effect variance/correlation estimates might look very different.  

### Try different scaling and centering of opponent size
At the risk hamnmering this point harder than necessary, let's fit the exact same model but using opponent size in KG rather than SDU (in both fixed and random parts of the model), we'll include the focal body size in unscaled form as well now (i.e. it is also in KG)
```{r aggr_rs, results=FALSE}

#create another column which is opponent size in KG
haggis$opp_sizeKG<- (haggis$opp_size*sd(haggis$body_size))+mean(haggis$body_size)

#refit the model
lmer_4_rr <- lmer(aggression ~ opp_sizeKG + body_size + 
                 scale(assay_rep, scale = FALSE) + block + 
                  (opp_sizeKG|ID),
                  data = haggis)
              
```
This generates a warning about convergence. In reality this is coming because we accidentally simulated **super strong** slope intercept correlation in this data set. Just as variances are constrained to be positive, correlation estimates are (by default forced to be between -1 and +1). So ignore the warning (not a general life rule of course) and summarise model `lmer_4_rr`.   

**What has changed? What has not? Why?**  
Compare fixed and random effects and think about the conclusions you would might draw from these two versions of the same model. You can always use `summary(lmer_3_rr)` to get the previous results. in particular think about:    
* Why the fixed intercept is different. Is it still the overall mean aggressiveness?  
* What has happened to random intercept variance. Has it changed in size? How do we interpret it?   
* Look at the sign of the slope-intercept correlation. Has it changed? Why?  
  
In reality of course these models are identical - look at the predictions over the range of actually observed opponent size and it should be familiar.

```{r plot_aggr_rs, fig.height=4, fig.width=5, fig.align='center'}

augment(lmer_4_rr) %>% 
  select(ID, block, opp_sizeKG, .fitted, aggression) %>% 
  filter(block == -0.5) %>% 
  gather(type,aggression,
         `.fitted`:aggression) %>% 
  ggplot(., aes(x = opp_sizeKG, y = aggression, group = ID)) +
  geom_line(alpha = 0.3) +
  theme_classic() +
  facet_grid(.~type)

```

This is very important! **A different scaling does not change model predictions** and will not change **correct** biological conclusions in any way. However, failure to understand the consequences of scaling and centering leads some researchers to interpret their models incorrectly. For instance, hopefully you now see that the intercept-slope correlation is highly dependent on where we set zero on the x axs. This is absolutely as it should be --- for example, using opponent sizes in KG then x=0 would be off to the far left of any actual data (try `summary(haggis$opp_sizeKG)`), and the intercept-slope correlation goes to -1 as the least positive intercept deviations have the most positive slope at x=0KG.

## 6. A character state view of phenotypic plasticity
If we can't readily compare the magnitudes of variance in intercept (among individual variation in mean aggressiveness at X=0) with variance in slopes (among-individual variance in plasticity, IxE) how do we go beyond simply saying we have evidence that IxE is present. How can we give a feel for its importance? Our view is that this is often better done by considering the implications of IxE detected under a *'character state'* rather than *'reaction norm'* perspective.

### IxE means among-individual variance changes across environmental conditions 
In generally we start out using repeat measures in a behavioural study because we want to know about the magnitude of variation among individuals. In the simple random intercept model we assume that any individual *i*'s deviation from the fixed effect effect mean is constant with *x* - it is the same every time we observe that individual regardless of environment. If individual deviations can't change, neither can the variance of these deviations (i.e. the among-individual variance). In contrast, under our random regression model we allow the deviations to change as a linear function of the environment. This means variance can change with environment, it also means the correlation between individual deviations in different environments need not be perfect (+1), it could be much lower or even negative. 

Look again at the plotted predictions from either `lmer_3_rr` or `lmer_4_rr` recalling they are really the same model. You should see the model is predicting much more variation in focal aggressiveness at high opponent size. You should also see lots of reaction norms crossing. This means that in fact the most aggressive individuals against small opponents are actually expected to be among the least aggressive when challenged by large opponents. In other words the correlation of individual deviations across environmental contexts is not going to be +1, and will probably even be <0.   

### Create environment-specific traits or character states
We could explore the same idea by defining environment specific traits, and analysing them to see if the same patterns emerge. Is there really more among-individual variance when opponent size is high? The best way to do this would depend on your data structure, but here the two blocks means we actually have repeats (albeit only 2) on each individual within each environment.  

Create some environment specific data subsets
```{r cs_subsets}

haggis_s <- haggis[haggis$opp_size==-1,]
haggis_m <- haggis[haggis$opp_size==0,]
haggis_l <- haggis[haggis$opp_size==1,]
```
Try a mixed model of aggression at small opponent size, no random regression now, and no fixed effect of opponent size 

```{r aggr_cs, results=FALSE}
lmer_5 <- lmer(aggression ~ body_size + 
                 scale(assay_rep, scale = FALSE) + block + 
                  (1|ID),
                  data = haggis_s)
```

Summarise, and make a note of the estimated variance among individuals. Is it statistically significant (use `ranova`). Try the same model for the other two opponent sizes by changing the name of the selected data subset. Any support for an increasing pattern of Vind with opponent size (as predicted by the random regression model)? 

### What do these models lead you to conclude? 
How can we reconcile what they are telling you with the simple model of all data we started with, the one that suggested (almost) no repeatable variation when we viewed aggressivness across all environments as a single character? if you need a clue - it is provided by the crossing reaction norms apparent in the random regression model.

## 7. Towards a multivariate modelling strategy 

By formulating a trivariate model we could treat the 3 environment-specific characters as response variables in a single model. This would allow us to estimate - in one go - the three among-individual variances and all the among-individual covariances/correlations. To do this we would need to pull on our big girl/boy pants and learn how to fit multivariate mixed models. It's not very hard, and once you do it you will start to seem like a god to some univariate scientists trapped in flatland (they will be plying you with coffee/beer/authorships to help them with their analyses). It's a step too far for today though.

However, *if* we did this, we'd see that although there is repeatable variation within each environment (opponent size) but also strong negative correlation between individual deviation for aggressiveness with small opponents and individual performance against larger opponents. This is why the first models suggested little or no among-individual variance when we analysed data from the 3 environmental contexts all at once. It is true - that across the range of opponent sizes tested individuals do not perform consistently, however, in this case that doesn't mean aggressiveness is not repeatable within a single environment. 

### But you already did a multivariate model..*kind of*
This bit may hurt your head... but it's so cool once you get it. The random regression model actually predicts what individual specific deviations from the fixed effect mean are at each state of the environment (opponent size). So although we have modelled a slope and an intercept for each individual, given these we can predict the trait (aggressivness) at any chosen value of the environment (opponent size). If that's true, we shodl also be able to estimate the among-individual variance in each environment, and also the individual-level correlations across environments. 

Indeed we can do these things. It requires tiny bit of matrix algebra, but not much:

First lets get the summary of our model again, we'll use the one with scaled opponent weight as it's easier to interpret
```{r aggr_lmer_summary2, results=FALSE}
summary(lmer_3_rr)
```

The model output gives us estimates of individual-level intercept and slope variances and the correlation. We actually need the slope-intercept covariance rather than the correlation, and then we can put this a 2x2 square matrix that specifies the (co)variance structure of inidvidual intercepts and slopes. We'll call this **ID~rn~** with ID meaning haggis identity, and rn to denote reaction norm.   

```{r }
#since r=COV12/sqrt(V1*V2)
#COV12 = r*sqrt(V1*V2)

#from the summary
Vint<-0.05044
Vslp<-0.19167

#so
COVint.slp<-0.96*sqrt(Vint*Vslp)

#build a matrix

ID_rn<-as.matrix(cbind(c(Vint,COVint.slp),
                      c(COVint.slp, Vslp)))

ID_rn
```

We also need to make a second matrix, let's call it **Q** (no particular reason, pick something else if you want). This is going to contain the values needed to turn an individual's intercept (mean) and slope (plasticity) deviations into estimates of environment-specific individual merit in a character state model.  

What do we mean by this? Well if an individual i has an intercept deviation of ID~int(i)~ and a slope deviation of ID~slp(i)~ for a given value of the environment  `opp_size` we might be interested in:

ID~i~ = (1 x ID~int(i)~) + (`opp_size` x ID~slp(i)~)

We want to look at character states representing the three observed values of `opp_size` here so 

```{r }
Q<-as.matrix(cbind(c(1,1,1),
                  c(-1,0,1)))
```

Then we can generate our among-individual covariance matrix environment specific aggresiveness, which we can call **ID~cs~** by matrix multiplication:  
```{r }
ID_cs<- Q %*% ID_rn %*%t(Q)    #where t(Q) is the transpose of Q 
                               #and %*% is matrix multiplication 

ID_cs  #rows and columns correspond to aggressiveness at opp_size=-1,0,1 in that order


cov2cor(ID_cs)   #Converting to a correlation scale 

```





